{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import call_tracing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA, FactorAnalysis, FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS, SpectralEmbedding, TSNE, Isomap\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import OPTICS, DBSCAN, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import umap\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 10\n",
    "use_log = True\n",
    "use_std = True\n",
    "# ignore the first day\n",
    "ignored = 1440\n",
    "\n",
    "# group_func = 'mean'\n",
    "group_func = 'mean_and_median'\n",
    "\n",
    "nn = 50\n",
    "\n",
    "reducers = {\n",
    "    'pca': PCA(n_components=3),\n",
    "    # 'umap': umap.UMAP(n_components=3, metric='cosine', n_neighbors=5, random_state=5566),\n",
    "    'umap': umap.UMAP(n_components=3, metric='cosine', n_neighbors=nn, random_state=5566),\n",
    "    # 'umap': umap.UMAP(n_components=3, n_neighbors=15, random_state=5566),\n",
    "}\n",
    "# reducer_name = 'pca'\n",
    "reducer_name = 'umap'\n",
    "reducer = reducers[reducer_name]\n",
    "# custom_profile = 'nocustom'\n",
    "custom_profile = 'cosine-nn%d' % nn\n",
    "profile = '_'.join([reducer_name, 'log' if use_log else 'nolog', group_func, 'std' if use_std else 'nostd', 'ignore%d' % ignored, custom_profile])\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_size = 3\n",
    "txts = [f for f in os.listdir('./aligned') if f.endswith('.txt')]\n",
    "monitor_cleaned_smooths = []\n",
    "act_digests = []\n",
    "act_origs = []\n",
    "metas = []\n",
    "\n",
    "one_meta = pd.read_csv('./meta/one_meta.csv', sep='\\t')\n",
    "# txt = txts[0]\n",
    "for txt in txts:\n",
    "\n",
    "    # try:\n",
    "    #     meta = pd.read_csv('./meta/%s' % txt, sep='\\t')\n",
    "    # except:\n",
    "    #     continue\n",
    "\n",
    "    meta = one_meta[one_meta.File_Name == os.path.splitext(txt)[0]]\n",
    "    #meta = one_meta[one_meta.File_Name == 'sdfghjkl;']\n",
    "    if len(meta) == 0:\n",
    "        continue\n",
    "\n",
    "    monitor = pd.read_csv('./aligned/%s' % txt, sep='\\t', header=None)\n",
    "    monitor = monitor.rename({1:'date', 2:'time'}, axis=1)\n",
    "    monitor_cleaned = pd.concat([monitor.iloc[:,1:3], monitor.iloc[:,10:]], axis=1)\n",
    "    monitor_cleaned = monitor_cleaned.iloc[ignored:,:]\n",
    "    # monitor_cleaned_smooth = monitor_cleaned.iloc[:,2:].apply(np.convolve, v=np.array([1,1,1,1,1]), mode='valid')\n",
    "    monitor_cleaned_smooth = monitor_cleaned.iloc[:,2:].apply(np.convolve, v=np.ones(conv_size), mode='valid')\n",
    "    \n",
    "    if use_log:\n",
    "        monitor_cleaned_smooth = np.log(monitor_cleaned_smooth + 1)\n",
    "\n",
    "    monitor_cleaned_smooth = pd.concat([monitor_cleaned.iloc[(conv_size-1):,:2].reset_index(drop=True), monitor_cleaned_smooth], axis=1)\n",
    "\n",
    "    hms = np.array([t.replace(' ', ':').split(':') for t in monitor_cleaned_smooth.time], dtype=int)\n",
    "    monitor_cleaned_smooth['h'] = hms[:,0]\n",
    "    monitor_cleaned_smooth['mNcell'] = hms[:,1] // win_size\n",
    "    #monitor_cleaned_smooth = monitor_cleaned_smooth[monitor_cleaned_smooth.h.isin([18,19,20,21,22,23,0,1,2,3,4,5])]\n",
    "    #monitor_cleaned_smooth['m'] = hms[:,1]\n",
    "    \n",
    "    if group_func == 'mean':\n",
    "        act_digest = pd.concat([monitor_cleaned_smooth.groupby(['h', 'mNcell']).mean().T, monitor_cleaned_smooth.groupby(['h', 'mNcell']).std().T], axis=1)\n",
    "    else:\n",
    "        # act_digest = pd.concat([monitor_cleaned_smooth.groupby(['h', 'mNcell']).median().T, monitor_cleaned_smooth.groupby(['h', 'mNcell']).std().T], axis=1)\n",
    "        q1 = monitor_cleaned_smooth.groupby(['h', 'mNcell']).apply(pd.DataFrame.quantile, q=.25).T.iloc[:-2]\n",
    "        q3 = monitor_cleaned_smooth.groupby(['h', 'mNcell']).apply(pd.DataFrame.quantile, q=.75).T.iloc[:-2]\n",
    "\n",
    "        monitor_cleaned_smooth_min = monitor_cleaned_smooth.groupby(['h', 'mNcell']).min().T.iloc[2:]\n",
    "        monitor_cleaned_smooth_max = monitor_cleaned_smooth.groupby(['h', 'mNcell']).max().T.iloc[2:]\n",
    "        IQR = q3 - q1\n",
    "        monitor_cleaned_smooth_whisker_min = q1 - 1.5 * IQR\n",
    "        monitor_cleaned_smooth_whisker_max = q3 + 1.5 * IQR\n",
    "        \n",
    "        whisker_min_oob = (monitor_cleaned_smooth_whisker_min < monitor_cleaned_smooth_min)\n",
    "        whisker_max_oob = (monitor_cleaned_smooth_whisker_max > monitor_cleaned_smooth_max)\n",
    "        monitor_cleaned_smooth_whisker_min[whisker_min_oob] = monitor_cleaned_smooth_min[whisker_min_oob]\n",
    "        monitor_cleaned_smooth_whisker_max[whisker_max_oob] = monitor_cleaned_smooth_max[whisker_max_oob]\n",
    "        \n",
    "        act_digest = pd.concat([\n",
    "            monitor_cleaned_smooth.groupby(['h', 'mNcell']).mean().T, \n",
    "            monitor_cleaned_smooth.groupby(['h', 'mNcell']).std().T,\n",
    "            monitor_cleaned_smooth.groupby(['h', 'mNcell']).median().T, \n",
    "            q1,\n",
    "            q3,\n",
    "            monitor_cleaned_smooth_whisker_min,\n",
    "            monitor_cleaned_smooth_whisker_max,\n",
    "            ], axis=1)\n",
    "    \n",
    "    act_orig = monitor_cleaned_smooth.iloc[:,2:34].T\n",
    "    #act_digest = monitor_cleaned_smooth.groupby(['h', 'mNcell']).mean().T / monitor_cleaned_smooth.groupby(['h', 'mNcell']).std().T\n",
    "    #act_digest = monitor_cleaned_smooth.groupby(['h', 'mNcell']).mean().T\n",
    "    act_digests.append(act_digest)\n",
    "    act_origs.append(act_orig)\n",
    "\n",
    "    metas.append(meta)\n",
    "    monitor_cleaned_smooths.append(monitor_cleaned_smooth)\n",
    "\n",
    "meta_union = pd.concat(metas).reset_index(drop=True)\n",
    "np.sum(meta_union.Instar.isna())\n",
    "act_digests_npy = np.concatenate(act_digests)\n",
    "act_origs_npy = np.concatenate(act_origs)\n",
    "monitor_cleaned_smooths_union = pd.concat(monitor_cleaned_smooths).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=3, metric='cosine', n_neighbors=nn, random_state=5566)\n",
    "\n",
    "try:\n",
    "    pwdists = pairwise_distances(dr)\n",
    "except:\n",
    "    act_digests_npy = np.concatenate(act_digests)\n",
    "    act_digests_npy_transformed = stdscaler(act_digests_npy, use_std)\n",
    "    dr = reducer.fit_transform(act_digests_npy_transformed)\n",
    "    act_digests_npy_transformed.shape\n",
    "    dr.shape\n",
    "    # reducer.fit(act_digests_npy_transformed)\n",
    "    # reducer.transform(act_digests_npy_transformed)\n",
    "    pwdists = pairwise_distances(dr)\n",
    "\n",
    "pwdists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 20\n",
    "\n",
    "shortest_dists_mean = np.take_along_axis(pwdists, np.argsort(pwdists)[:,1:(1+min_samples)], axis=1).mean(axis=1)\n",
    "shortest_dists_mean_std = shortest_dists_mean.std()\n",
    "eps = shortest_dists_mean.mean() + 2 * shortest_dists_mean_std\n",
    "\n",
    "clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "group_idxs_ = clusterer.fit_predict(dr) + 1\n",
    "group_idxs = group_idxs_[group_idxs_ > 0]\n",
    "dr_wg = dr[group_idxs_ > 0]\n",
    "gcolor_map = group_idxs / (group_idxs.max() + 1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "axs[0][0].scatter(dr_wg[:,0], dr_wg[:,1], c=gcolor_map)\n",
    "axs[0][1].scatter(dr_wg[:,2], dr_wg[:,1], c=gcolor_map)\n",
    "axs[1][0].scatter(dr_wg[:,0], dr_wg[:,2], c=gcolor_map)\n",
    "for tid in range(dr.shape[0]):\n",
    "    axs[0][0].text(dr_wg[tid,0], dr_wg[tid,1], group_idxs[tid])\n",
    "    axs[0][1].text(dr_wg[tid,2], dr_wg[tid,1], group_idxs[tid])\n",
    "    axs[1][0].text(dr_wg[tid,0], dr_wg[tid,2], group_idxs[tid])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "instar_values = meta_union.Instar.values\n",
    "instar_values[meta_union.Instar.isna()] = -1\n",
    "instar_values = instar_values.astype(int)\n",
    "\n",
    "gcolor_map_ = (instar_values+1) / (instar_values.max() + 1)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "axs[0][0].scatter(dr[:,0], dr[:,1], c=gcolor_map_)\n",
    "axs[0][1].scatter(dr[:,2], dr[:,1], c=gcolor_map_)\n",
    "axs[1][0].scatter(dr[:,0], dr[:,2], c=gcolor_map_)\n",
    "for tid in range(dr.shape[0]):\n",
    "    axs[0][0].text(dr[tid,0], dr[tid,1], instar_values[tid]+1)\n",
    "    axs[0][1].text(dr[tid,2], dr[tid,1], instar_values[tid]+1)\n",
    "    axs[1][0].text(dr[tid,0], dr[tid,2], instar_values[tid]+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = meta_union.Instar.values\n",
    "y_[meta_union.Instar.isna()] = -1\n",
    "y = y_[y_ != -1]\n",
    "len(y)\n",
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = meta_union[['Source', 'Gen', 'Sex']].copy()\n",
    "x_['group'] = group_idxs_\n",
    "x = x_[y_ != -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_union[x_.group==3]\n",
    "meta_union.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order for feature_importances_ to work, should only run rfc = RandomForestClassifier()\n",
    "rfc = RandomForestClassifier(random_state=42, max_depth=4, n_estimators=5000, criterion='entropy')\n",
    "rfc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "rfc = SVC(kernel='linear')\n",
    "rfc = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False).fit(x)\n",
    "new_x = enc.transform(x)\n",
    "new_x.shape\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_x, y, test_size=.5, random_state=42)\n",
    "# rfc.fit(x_train, y_train)\n",
    "# rfc.score(x_test, y_test)\n",
    "# rfc.score(x_train, y_train)\n",
    "# rfc.feature_importances_\n",
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(x_train, y_train)\n",
    "accuracy_score(y_train, rfc.predict(x_train))\n",
    "rfc.score(x_test, y_test)\n",
    "accuracy_score(y_test, rfc.predict(x_test))\n",
    "rfc.feature_importances_\n",
    "np.argsort(np.abs(rfc.coef_))\n",
    "np.argsort(rfc.feature_importances_)\n",
    "\n",
    "np.concatenate(enc.categories_, axis=0)[np.argsort(np.abs(rfc.coef_))][:,-5:]\n",
    "np.concatenate(enc.categories_, axis=0)[np.argsort(rfc.feature_importances_)][-5:]\n",
    "\n",
    "meta_union[meta_union.Source=='OK']\n",
    "\n",
    "x_test[:,2].astype(int)\n",
    "y_test\n",
    "\n",
    "np.sum(x_test[:,3] == y_test) / len(y_test)\n",
    "np.sum(x_test[:,3] != y_test) / len(y_test)\n",
    "\n",
    "np.sum(new_x[:,3] == y) / len(y)\n",
    "\n",
    "meta_union.Instar\n",
    "\n",
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def packing(df, bug_col = 2):\n",
    "    window_acts_per_bug.append(df.iloc[:,bug_col].values)\n",
    "    return None\n",
    "\n",
    "# for bug_col in range(2, 34):\n",
    "bug_col=33\n",
    "window_acts_per_bug = []\n",
    "dum = monitor_cleaned_smooths_union.groupby(['h', 'mNcell']).apply(packing, bug_col=bug_col)\n",
    "plt.boxplot(window_acts_per_bug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cand_cols = ['LD_cycle', 'Nest', 'Source', 'Location', 'Elevation', 'Gen', 'Sex', 'Photo']\n",
    "# cand_cols = ['Nest', 'Source', 'Elevation', 'Gen', 'Sex', 'Photo']\n",
    "cand_cols = ['Source', 'Gen']\n",
    "cand_cols = ['Source']\n",
    "# cand_cols = ['LD_cycle', 'Source', 'Photo']\n",
    "# np.array(np.meshgrid(cand_cols, cand_cols)).T.reshape(-1, 2)\n",
    "for i in range(len(cand_cols)):\n",
    "    for j in range(i, len(cand_cols)):\n",
    "        cat_col = list(np.unique([cand_cols[i], cand_cols[j]]))\n",
    "        print(cat_col)\n",
    "\n",
    "        meta_union = pd.concat(metas).reset_index(drop=True)\n",
    "        act_digests_npy = np.concatenate(act_digests)\n",
    "\n",
    "        # filtered_idx = meta_union.Source.isin(['BF', 'MF'])\n",
    "        # # filtered_idx = meta_union.Source.isin(['WL'])\n",
    "        # # filtered_idx = meta_union.Source.isin(['WL'])\n",
    "        # meta_union = meta_union[filtered_idx]\n",
    "        # act_digests_npy = act_digests_npy[filtered_idx]\n",
    "\n",
    "        # cat_col = ['Source', 'Photo', 'LD_cycle']\n",
    "        # cat_col = ['Source']\n",
    "        # cat_col = ['Source', 'Photo']\n",
    "        cat_col_str = '_x_'.join(cat_col)\n",
    "        meta_union[cat_col_str] = meta_union[cat_col].replace(np.nan, 'NaN').astype(str).apply('_x_'.join, axis=1)\n",
    "\n",
    "        # meta_union.groupby(cat_col).size()\n",
    "\n",
    "        cat_list, cat_idxs = np.unique(meta_union[cat_col_str].values, return_inverse=True)\n",
    "        color_map = cat_idxs / (cat_idxs.max() + 1)\n",
    "        act_digests_npy_transformed = stdscaler(act_digests_npy, use_std)\n",
    "\n",
    "        dr = reducer.fit_transform(act_digests_npy_transformed)\n",
    "        # dr = dr[group_idxs_ > 0]\n",
    "        # color_map = color_map[group_idxs_ > 0]\n",
    "        # cat_idxs = cat_idxs[group_idxs_ > 0]\n",
    "\n",
    "        # ax = scatter3d(dr, color_map)\n",
    "        # for tid in range(dr.shape[0]):\n",
    "        #     ax.text3D(dr[tid, 0], dr[tid, 1], dr[tid, 2], cat_idxs[tid])\n",
    "        # plt.show()\n",
    "        # plt.savefig('./explore/%s.png' % cat_col_str)\n",
    "        # plt.close()\n",
    "\n",
    "        if dr.shape[1] >= 3:\n",
    "            print(cat_list)\n",
    "            \n",
    "            fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "            # print(axs)\n",
    "            \n",
    "            axs[0][0].scatter(dr[:,0], dr[:,1], c=color_map)\n",
    "            axs[0][1].scatter(dr[:,2], dr[:,1], c=color_map)\n",
    "            axs[1][0].scatter(dr[:,0], dr[:,2], c=color_map)\n",
    "            for tid in range(dr.shape[0]):\n",
    "                axs[0][0].text(dr[tid,0], dr[tid,1], cat_idxs[tid])\n",
    "                axs[0][1].text(dr[tid,2], dr[tid,1], cat_idxs[tid])\n",
    "                axs[1][0].text(dr[tid,0], dr[tid,2], cat_idxs[tid])\n",
    "            try:\n",
    "                os.makedirs('./explore/%s/' % profile,)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            plt.savefig('./explore/%s/%s.png' % (profile, cat_col_str))\n",
    "            plt.close()\n",
    "            # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "group_sizes = pd.DataFrame({'gid':group_idxs}).groupby('gid').size().values\n",
    "f_exp = group_sizes / group_sizes.sum()\n",
    "\n",
    "# cand_cols = ['Nest', 'Source', 'Elevation', 'Gen', 'Sex', 'Photo']\n",
    "cand_cols = ['Source', 'Gen', 'Sex', 'Photo']\n",
    "cand_cols = ['Instar']\n",
    "cand_cols = ['Source']\n",
    "# cand_cols = ['LD_cycle', 'Source', 'Photo']\n",
    "# np.array(np.meshgrid(cand_cols, cand_cols)).T.reshape(-1, 2)\n",
    "biased_chars_all = np.array([])\n",
    "chi2_all = np.array([])\n",
    "pvalue_all = np.array([])\n",
    "char_sample_size_all = np.array([])\n",
    "cat_col_str_all = np.array([])\n",
    "for i in range(len(cand_cols)):\n",
    "    for j in range(i, len(cand_cols)):\n",
    "        cat_col = list(np.unique([cand_cols[i], cand_cols[j]]))\n",
    "\n",
    "        meta_union = pd.concat(metas).reset_index(drop=True)\n",
    "        act_digests_npy = np.concatenate(act_digests)\n",
    "\n",
    "        cat_col_str = '_x_'.join(cat_col)\n",
    "\n",
    "        print(cat_col_str)\n",
    "        meta_union[cat_col_str] = meta_union[cat_col].replace(np.nan, 'NaN').astype(str).apply('_x_'.join, axis=1)\n",
    "\n",
    "        char_to_group = pd.DataFrame.from_dict({'gid': group_idxs, 'char': meta_union[cat_col_str].values[group_idxs_ > 0]}).pivot_table(index='gid', columns='char', aggfunc=len)\n",
    "        char_to_group = char_to_group.replace(np.nan, 0)\n",
    "\n",
    "        f_exp_weighted = char_to_group.sum().values * np.repeat(np.expand_dims(f_exp, axis=0), char_to_group.shape[1], axis=0).T\n",
    "        # pd.concat([char_to_group.reset_index(), pd.DataFrame(f_exp)], axis=1)\n",
    "        chi2test = chisquare(char_to_group, f_exp=f_exp_weighted)\n",
    "        \n",
    "        # group_sizesT = meta_union[[cat_col_str]].groupby(cat_col_str).size().values\n",
    "        # f_expT = group_sizesT / group_sizesT.sum()\n",
    "        # chi2testT = chisquare(char_to_group.T, f_exp=np.repeat(np.expand_dims(f_expT, axis=0), char_to_group.shape[0], axis=0).T)\n",
    "        # if (chi2testT.pvalue < 1e-4).all():\n",
    "        #     print(cat_col_str)\n",
    "        \n",
    "        pvalue_thres_idxs = (chi2test.pvalue < 0.05)\n",
    "        biased_chars = char_to_group.columns.values[pvalue_thres_idxs]\n",
    "        biased_chars_all = np.append(biased_chars_all, biased_chars)\n",
    "        chi2 = chi2test.statistic[pvalue_thres_idxs]\n",
    "        chi2_all = np.append(chi2_all, chi2)\n",
    "        pvalue_all = np.append(pvalue_all, chi2test.pvalue[pvalue_thres_idxs])\n",
    "        char_sample_size_all = np.append(char_sample_size_all, char_to_group.sum()[pvalue_thres_idxs].values)\n",
    "        cat_col_str_all = np.append(cat_col_str_all, np.repeat(cat_col_str, chi2.shape[0]))\n",
    "        print(biased_chars)\n",
    "        # print([d for d in char_to_group.columns.values if d not in biased_chars])\n",
    "\n",
    "biased_chars_all\n",
    "biased_chars_all[np.argsort(chi2_all)[-5:]]\n",
    "pvalue_all[np.argsort(chi2_all)[-5:]]\n",
    "char_sample_size_all\n",
    "\n",
    "pd.DataFrame({\n",
    "    'char_name': np.array(cat_col_str_all)[char_sample_size_all > 20],\n",
    "    'char_val': biased_chars_all[char_sample_size_all > 20],\n",
    "    'pvalue': pvalue_all[char_sample_size_all > 20],\n",
    "    'size': char_sample_size_all[char_sample_size_all > 20],\n",
    "})\n",
    "\n",
    "cat_col = ['Photo', 'Source']\n",
    "meta_union = pd.concat(metas).reset_index(drop=True)\n",
    "act_digests_npy = np.concatenate(act_digests)\n",
    "\n",
    "cat_col_str = '_x_'.join(cat_col)\n",
    "print(cat_col_str)\n",
    "meta_union[cat_col_str] = meta_union[cat_col].replace(np.nan, 'NaN').astype(str).apply('_x_'.join, axis=1)\n",
    "char_to_group = pd.DataFrame.from_dict({'gid': group_idxs, 'char': meta_union[cat_col_str].values}).pivot_table(index='gid', columns='char', aggfunc=len)\n",
    "char_to_group = char_to_group.replace(np.nan, 0)\n",
    "char_to_group\n",
    "f_exp\n",
    "\n",
    "monitor_cleaned_smooth\n",
    "meta_union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
